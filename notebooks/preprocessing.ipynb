{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21197d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line if running in Google Colab\n",
    "# !pip install clinicadl==1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545460bd",
   "metadata": {},
   "source": [
    "# Prepare your neuroimaging data\n",
    "\n",
    "There are different steps to perform before training your model or performing\n",
    "classification. In this notebook, we will see how to:\n",
    "\n",
    "1. **Organize** your neuroimaging data.\n",
    "2. **Preprocess** your neuroimaging data.\n",
    "3. Check the preprocessing **quality**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9d1d9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Organization of neuroimaging data: the Brain Imaging Data Structure (BIDS)\n",
    "\n",
    "Before processing your neuroimaging data, several steps may be needed. These\n",
    "steps can include converting the images to a format readable by neuroimaging\n",
    "software tools (e.g. converting to NIfTI) and organizing your files in a\n",
    "specific way. Several tools will require that your clinical and imaging data\n",
    "follow the **Brain Imaging Data Structure (BIDS)** [(Gorgolewski et al.,\n",
    "2016)](https://doi.org/10.1038/sdata.2016.44). The BIDS standard is based on a\n",
    "file hierarchy rather than on a database management system, thus facilitating\n",
    "its deployment. Thanks to its clear and simple way to describe neuroimaging\n",
    "and behavioral data, it has been easily adopted by the neuroimaging community.\n",
    "Organizing a dataset following the BIDS hierarchy simplifies the execution of\n",
    "neuroimaging software tools.  \n",
    "\n",
    "Here is a general overview of the BIDS structure. If you need more details,\n",
    "please check the\n",
    "[documentation](https://bids-specification.readthedocs.io/en/latest/) on the\n",
    "[website](http://bids.neuroimaging.io/).\n",
    "\n",
    "<pre>\n",
    "BIDS_Dataset/\n",
    "├── participants.tsv\n",
    "├── sub-CLNC01/\n",
    "│   │   ├── ses-M000/\n",
    "│   │   │   └── anat/\n",
    "│   │   │       └── <b>sub-CLNC01_ses-M000_T1w.nii.gz</b>\n",
    "│   │   └── sub-CLNC01_sessions.tsv\n",
    "├── sub-CLNC02/\n",
    "│   │   ├── ses-M000/\n",
    "│   │   │   ├── anat/\n",
    "│   │   │   │   └── <b>sub-CLNC02_ses-M000_T1w.nii.gz</b>\n",
    "│   │   │   └── pet/\n",
    "│   │   │       └── <b>sub-CLNC02_ses-M000_trc-18FFDG.nii.gz</b>\n",
    "│   │   └── sub-CLNC02_sessions.tsv\n",
    "└──  ...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d24d9",
   "metadata": {},
   "source": [
    "\n",
    "## `clinica convert` pipelines\n",
    "\n",
    "Both OASIS and ADNI dataset contain imaging data in ANALYZE format \n",
    "and do not provide a BIDS version of the data. To solve this issue, \n",
    "[clinica](https://aramislab.paris.inria.fr/clinica/docs/public/latest/) \n",
    "provides a [converter](https://aramislab.paris.inria.fr/clinica/docs/public/latest/Converters/OASIS2BIDS/)\n",
    "to automatically convert ANALYZE files into NIfTI following the BIDS \n",
    "standard.\n",
    "\n",
    "A command line instruction is enough to get the data in BIDS format:\n",
    "\n",
    "```bash\n",
    "clinica convert oasis-to-bids <dataset_directory> <clinical_data_directory> <bids_directory>\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "  - `dataset_directory` is the path to the original OASIS images' directory;\n",
    "  - `clinical_data_directory` is the path to the directory containing the\n",
    "  `oasis_cross-sectional.csv` file;\n",
    "  - `bids_directory` is the path to the output directory, where the\n",
    "  BIDS-converted version of OASIS will be stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b0c06",
   "metadata": {},
   "source": [
    "### Before starting\n",
    "We are going to run some experiments on the ADNI and OASIS datasets,\n",
    "if you have already downloaded the full dataset, you can set the\n",
    "path to your own directory when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c964a259",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Run the pipeline\n",
    "To run this pipeline, you need clinical data. The next cell allows you to \n",
    "download an example dataset with 4 images from OASIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b401f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the example dataset of 4 images\n",
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_oasis/database.tar.gz -o oasis_database.tar.gz\n",
    "!tar xf oasis_database.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aef766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the example dataset to BIDS\n",
    "!clinica convert oasis-to-bids data_oasis/database/RawData data_oasis/database/ClinicalData data_oasis/BIDS_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e264314",
   "metadata": {},
   "source": [
    "\n",
    "**Clinica** also provides other converters that work in the same way, \n",
    "such as:\n",
    "\n",
    "[adni-to-bids](https://aramislab.paris.inria.fr/clinica/docs/public/v0.7.2/Converters/ADNI2BIDS/), [aibl-to-bids](https://aramislab.paris.inria.fr/clinica/docs/public/v0.7.2/Converters/AIBL2BIDS/), [habs-to-bids](https://aramislab.paris.inria.fr/clinica/docs/public/v0.7.2/Converters/HABS2BIDS/), [nifd-to-bids](https://aramislab.paris.inria.fr/clinica/docs/public/v0.7.2/Converters/NIFD2BIDS/), [oasis3-to-bids](https://aramislab.paris.inria.fr/clinica/docs/public/v0.7.2/Converters/OASIS3TOBIDS/), [ukb-to-bids](https://aramislab.paris.inria.fr/clinica/docs/public/v0.7.2/Converters/UKBtoBIDS/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0767e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "(If you failed to obtain the BIDS example using the `oasis-to-bids`\n",
    "pipeline, please uncomment the next cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a5ccd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_oasis/BIDS_example.tar.gz -o BIDS_example.tar.gz\n",
    "!tar xf BIDS_example.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9b892",
   "metadata": {},
   "source": [
    "# Why prepare data ?\n",
    "Preprocessing of neuroimaging data is essential before doing any experiment \n",
    "and especially before training a neural network with that data.\n",
    "- **Registration** helps to standardize the neuroimaging data so that it is \n",
    "consistent across different subjects, scanners, and imaging modalities. This \n",
    "makes it easier for the deep neural network to learn patterns and make\n",
    "accurate predictions. \n",
    "- Preprocessing techniques such as **motion correction** and **noise\n",
    "reduction** can help to minimize sources of noise and improve the quality of \n",
    "the data. Neuroimaging data can be noisy due to a variety of factors, such as \n",
    "head motion, scanner artifacts, and biological variability. \n",
    "- Preprocessing can also be used to **extract features** from the neuroimaging\n",
    "data that are relevant to the task at hand. For example, if the goal is to\n",
    "classify brain regions based on their functional connectivity, preprocessing\n",
    "may involve computing correlation matrices from the fMRI time series data. \n",
    "- **Normalization** is another important preprocessing step for neuroimaging\n",
    "data that can help improve the performance of deep neural networks. \n",
    "\n",
    "Overall, preprocessing is essential in preparing neuroimaging data for deep\n",
    "neural network training. By standardizing and improving the quality of the\n",
    "data, these steps help to ensure that the deep neural network can learn\n",
    "meaningful patterns and make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8bcedc",
   "metadata": {},
   "source": [
    "\n",
    "Although convolutional neural networks (CNNs) have the potential to extract\n",
    "low-to-high level features from raw images, a proper image preprocessing\n",
    "procedure is fundamental to ensure a good classification performance (in\n",
    "particular for Alzheimer's disease (AD) classification where datasets are\n",
    "relatively small).  In the context of deep learning-based classification,\n",
    "image preprocessing procedures often include:\n",
    "- **Bias field correction:** MR images can be corrupted by a low frequency and\n",
    "smooth signal caused by magnetic field inhomogeneities. This bias field\n",
    "induces variations in the intensity of the same tissue in different locations \n",
    "of the image, which deteriorates the performance of image analysis algorithms\n",
    "such as registration.\n",
    "- **Image registration:** Medical image registration consists of spatially\n",
    "aligning two or more images, either globally (rigid and affine registration)\n",
    "or locally (non-rigid registration), so that voxels in corresponding positions\n",
    "contain comparable information.\n",
    "- **Cropping**: some specific regions of the registered images are selected in\n",
    "order to remove the background and to reduce the computing power required\n",
    "when training deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3954e768",
   "metadata": {},
   "source": [
    "This notebook presents three possible preprocessing steps using the [Clinica](https://www.clinica.run/doc/)\n",
    "software: \n",
    "- `t1-linear`: Affine registration of T1w images to the MNI standard space,\n",
    "- `t1-volume`: Volume-based processing of T1w images with SPM,\n",
    "- `pet-linear`: Spatial normalization to the MNI space and intensity\n",
    "normalization of PET images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731a32d",
   "metadata": {},
   "source": [
    "<a id='preprocessing:t1-linear'></a>\n",
    "## Image preprocessing with the `t1-linear` pipeline\n",
    "For this tutorial, we propose a \"minimal preprocessing\" (as described in [(Wen\n",
    "et al., 2020)](https://doi.org/10.1016/j.media.2020.101694)) implemented in\n",
    "the [`t1-linear` pipeline](https://aramislab.paris.inria.fr/clinica/docs/public/latest/Pipelines/T1_Linear/)\n",
    "using the [ANTs](http://stnava.github.io/ANTs/) software package [(Avants et\n",
    "al., 2014)](https://doi.org/10.3389/fninf.2014.00044). This preprocessing\n",
    "includes:\n",
    "- **Bias field correction** using the N4ITK method [(Tustison et al.,\n",
    "2010)](https://doi.org/10.1109/TMI.2010.2046908)\n",
    "- **Affine registration** to the MNI152NLin2009cSym template (Fonov et al.,\n",
    "[2011](https://doi.org/10.1016/j.neuroimage.2010.07.033), \n",
    "[2009](https://doi.org/10.1016/S1053-8119(09)70884-5) ) in MNI space with the\n",
    "SyN algorithm [(Avants et al.,\n",
    "2008)](https://doi.org/10.1016/j.media.2007.06.004).\n",
    "- **Cropping** resulting in final images of size 169×208×179 with 1 mm3\n",
    "isotropic voxels.\n",
    "\n",
    "If you run this notebook locally, please check that ANTs is correctly\n",
    "installed. If it is not the case, uncomment the three following lines and run\n",
    "it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/bin/bash -c \"$(curl -k https://aramislab.paris.inria.fr/files/software/scripts/install_conda_ants.sh)\"\n",
    "# # from os import environ\n",
    "# # environ['ANTSPATH']=\"/usr/local/bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd7a71",
   "metadata": {},
   "source": [
    "These steps can be run with this simple command line:\n",
    "```bash\n",
    "  clinica run t1-linear <bids_directory> <caps_directory>\n",
    "```\n",
    "where:\n",
    "\n",
    "- `bids_directory` is the input folder containing the dataset in a\n",
    "[BIDS](https://aramislab.paris.inria.fr/clinica/docs/public/latest/BIDS/) hierarchy,\n",
    "- `caps_directory` is the output folder containing the results in a\n",
    "[CAPS](https://aramislab.paris.inria.fr/clinica/docs/public/latest/CAPS/Introduction/) hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be4ed8b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "```{note}\n",
    "The following command can take some time to execute, depending on the\n",
    "configuration of your host machine. Running in a classical **Colab** instance\n",
    "can take up to 30 min.\n",
    "\n",
    "We will increase a little bit the computation capacity using 2 cores with the\n",
    "`--n_procs 2` flag. Since there are 4 images, you can set `--n_procs 4` if\n",
    "your computer can handle this.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c6ac7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a850ce36",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!clinica run t1-linear data_oasis/BIDS_example data_oasis/CAPS_example --n_procs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5e853",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Once the pipeline has been run, the necessary outputs for the next steps are\n",
    "saved using a specific suffix:\n",
    "`_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5d89c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "(If you failed to obtain the preprocessing using the `t1-linear` pipeline,\n",
    "please uncomment the next cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c5d95",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# !curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_oasis/CAPS_example.tar.gz -o CAPS_example.tar.gz\n",
    "# !tar xf CAPS_example.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef2a36",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "```{warning}\n",
    "The registration algorithm provided by ANTs exposes some reproducibility issues\n",
    "when running in different environments. The outputs are \"visually\" very close\n",
    "but not exactly the same. For further information and some clues on how to\n",
    "reduce the variability when using ANTs please read its\n",
    "[documentation page](https://github.com/ANTsX/ANTs/wiki/antsRegistration-reproducibility-issues).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d9174a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "For example, we can see the difference between raw images and processed images\n",
    "from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "suffix_caps = \"_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz\"\n",
    "suffix_bids = \"_T1w.nii.gz\"\n",
    "sub1 = f\"data_oasis/BIDS_example/sub-OASIS10016/ses-M000/anat/sub-OASIS10016_ses-M000{suffix_bids}\"\n",
    "sub2 = f\"data_oasis/CAPS_example/subjects/sub-OASIS10016/ses-M000/t1_linear/sub-OASIS10016_ses-M000{suffix_caps}\"\n",
    "sub3 = f\"data_oasis/BIDS_example/sub-OASIS10304/ses-M000/anat/sub-OASIS10304_ses-M000{suffix_bids}\"\n",
    "sub4 = f\"data_oasis/CAPS_example/subjects/sub-OASIS10304/ses-M000/t1_linear/sub-OASIS10304_ses-M000{suffix_caps}\"\n",
    "\n",
    "plotting.plot_anat(sub3, title=\"raw data: sub-OASIS10304\")\n",
    "plotting.plot_anat(sub4, title=\"preprocessed data: sub-OASIS10304\")\n",
    "plotting.plot_anat(sub1, title=\"raw data: sub-OASIS10016\")\n",
    "plotting.plot_anat(sub2, title=\"preprocessed data: sub-OASIS10016\")\n",
    "\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf410ed",
   "metadata": {},
   "source": [
    "<a id='preprocessing:pet-linear'></a>\n",
    "## Image preprocessing with the `pet-linear` pipeline\n",
    "\n",
    "This pipeline performs spatial normalization to the MNI space and intensity\n",
    "normalization of PET images. Its steps include:\n",
    "- **Affine registration** to the MNI152NLin2009cSym template \n",
    "[Fonov et al., [2011](https://www.sciencedirect.com/science/article/pii/S1053811910010062?via%3Dihub), \n",
    "[2009](https://www.sciencedirect.com/science/article/pii/S1053811909708845?via%3Dihub)] \n",
    "in MNI space with the SyN algorithm [[Avants et al., 2008]](https://doi.org/10.1016/j.media.2007.06.004) \n",
    "from the ANTs software package [[Avants et al., 2014]](https://doi.org/10.3389/fninf.2014.00044);\n",
    "- **Intensity normalization** using the average PET uptake in reference\n",
    "regions resulting in a standardized uptake value ratio (SUVR) map;\n",
    "- **Cropping** of the registered images to remove the background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b25f01",
   "metadata": {},
   "source": [
    "```{note}\n",
    "You need to have performed the t1-linear pipeline on your T1-weighted MR images.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65642a93",
   "metadata": {},
   "source": [
    "The pipeline can be run with the following command line:\n",
    "\n",
    "```bash\n",
    "  clinica run pet-linear [OPTIONS] BIDS_DIRECTORY CAPS_DIRECTORY ACQ_LABEL\n",
    "                       {pons|cerebellumPons|pons2|cerebellumPons2}\n",
    "````\n",
    "where:\n",
    "\n",
    "- `bids_directory` is the input folder containing the dataset in a\n",
    "[BIDS](https://aramislab.paris.inria.fr/clinica/docs/public/latest/BIDS/) hierarchy;\n",
    "- `caps_director` is the output folder containing the results in a\n",
    "[CAPS](https://aramislab.paris.inria.fr/clinica/docs/public/latest/CAPS/Introduction/) hierarchy;\n",
    "- `acq_label` is the label given to the PET acquisition, specifying the tracer\n",
    "used (trc-<acq_label>). It can be for instance '18FFDG' for\n",
    "18F-fluorodeoxyglucose or '18FAV45' for 18F-florbetapir;\n",
    "- The reference region is used to perform intensity normalization (i.e.\n",
    "dividing each voxel of the image by the average uptake in this region)\n",
    "resulting in a standardized uptake value ratio (SUVR) map. \n",
    "It can be cerebellumPons or cerebellumPons2 (used for amyloid tracers) and\n",
    "pons or pons2 (used for FDG). See [PET introduction](clinical) for more\n",
    "details about masks versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d13f1d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "```{warning}\n",
    "The following command can take some time to execute, depending on the\n",
    "configuration of your host machine. Running in a classical **Colab** instance\n",
    "can take up to 30 min.\n",
    "\n",
    "We will increase a little bit the computation capacity using 2 cores with the\n",
    "`--n_procs 2` flag. Since there are 4 images, you can set `--n_procs 4` if\n",
    "your computer can handle this.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010bfca2",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "Please uncomment the next cells to download a dataset of pet images of 4 \n",
    "subjects from ADNI in a BIDS format (convert to BIDS with `clinica convert adni-to-bids`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30360ff6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_adni/BIDS_example.tar.gz -o adniBids.tar.gz\n",
    "!tar xf adniBids.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362e758",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!clinica run t1-linear data_adni/BIDS_example data_adni/CAPS_example --n_procs 2\n",
    "!clinica run pet-linear data_adni/BIDS_example data_adni/CAPS_example 18FFDG cerebellumPons2 --n_procs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b2b59",
   "metadata": {},
   "source": [
    "Once the pipeline has been run, the necessary outputs for the next steps are\n",
    "saved using a specific suffix:\n",
    "`_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_suvr-<ref-region>_pet.nii.gz`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217792b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "If you failed to run the previous cell, please uncomment the next one to \n",
    "download the CAPS dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa87eee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "!curl -k https://aramislab.paris.inria.fr/clinicadl/files/handbook_2023/data_adni/CAPS_example.tar.gz -o adniCaps.tar.gz\n",
    "!tar xf adniCaps.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac96289",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "For example, we can see the difference between raw images and processed images\n",
    "from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "suffix_caps = \"_task-rest_trc-18FFDG_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_suvr-cerebellumPons2_pet.nii.gz\"\n",
    "suffix_bids = \"_task-rest_trc-18FFDG_pet.nii.gz\"\n",
    "sub1 = f\"data_adni/BIDS_example/sub-ADNI005S0610/ses-M072/pet/sub-ADNI005S0610_ses-M072{suffix_bids}\"\n",
    "sub2 = f\"data_adni/CAPS_example/subjects/sub-ADNI005S0610/ses-M072/pet_linear/sub-ADNI005S0610_ses-M072{suffix_caps}\"\n",
    "sub3 = f\"data_adni/BIDS_example/sub-ADNI005S0929/ses-M000/pet/sub-ADNI005S0929_ses-M000{suffix_bids}\"\n",
    "sub4 = f\"data_adni/CAPS_example/subjects/sub-ADNI005S0929/ses-M000/pet_linear/sub-ADNI005S0929_ses-M000{suffix_caps}\"\n",
    "\n",
    "plotting.plot_anat(sub3, title=\"raw data: sub-ADNI005S0929\")\n",
    "plotting.plot_anat(sub4, title=\"preprocessed data: sub-ADNI005S0929\")\n",
    "plotting.plot_anat(sub1, title=\"raw data: sub-ADNI005S0610\")\n",
    "plotting.plot_anat(sub2, title=\"preprocessed data: sub-ADNI005S0610\")\n",
    "\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac492e",
   "metadata": {},
   "source": [
    "# Quality check of your preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56324764",
   "metadata": {},
   "source": [
    "From the 2 visualizations above, we can see that after the preprocessing, some\n",
    "images have some missing skin voxels on top of the brain i.e. these images are\n",
    "slightly cropped.  Besides, we did not compare them to the [MNI152NLin2009cSym\n",
    "template](https://bids-specification.readthedocs.io/en/stable/99-appendices/08-coordinate-systems.html)\n",
    "to evaluate the quality of the registration.\n",
    "\n",
    "OASIS-1 dataset contains 416 images and ADNI more than 3000, so the quality\n",
    "check of the entire datasets can be very time-consuming. The next section gives\n",
    "you some ideas on how to keep only the images correctly preprocessed when \n",
    "working on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb08a9",
   "metadata": {},
   "source": [
    "To automatically assess the quality of the **t1-linear** or the **pet-linear** preprocessing, we\n",
    "propose to use a pretrained network which learnt to classify images that are\n",
    "adequately registered to a template from others for which the registration\n",
    "failed. This procedure is adapted from [(Fonov et al,\n",
    "2022)](https://doi.org/10.1016/j.neuroimage.2022.119266), using their\n",
    "pretrained models. The original code of [(Fonov et al,\n",
    "2022)](https://doi.org/10.1016/j.neuroimage.2022.119266) can be found on\n",
    "[GitHub](https://github.com/vfonov/DARQ).\n",
    "\n",
    "The quality check can be run with the following command line:\n",
    "```\n",
    "clinicadl quality-check <preprocessing> <caps_directory> <output_path>\n",
    "```\n",
    "where:\n",
    "\n",
    "- `preprocessing` corresponds to the preprocessing pipeline whose outputs will\n",
    "be checked (`t1-linear` or `pet-linear` or `t1-volume`),\n",
    "- `caps_directory` is the folder containing the results of the preprocessing\n",
    "pipeline in a [CAPS](https://aramislab.paris.inria.fr/clinica/docs/public/latest/CAPS/Introduction/) hierarchy,\n",
    "- `output_path` is the path to the output TSV file (or directory for\n",
    "`t1-volume`) containing QC results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0545ef5",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Quality checks pipelines are all different and depend on the chosen\n",
    "preprocessing. They should not be applied to other preprocessing procedures as\n",
    "the results may not be reliable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2869e8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01657786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality-check for t1-linear preprocessing\n",
    "!clinicadl quality-check t1-linear data_oasis/CAPS_example data_oasis/QC_result_t1.tsv --no-gpu --threshold 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a717e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality-check for pet-linear preprocessing\n",
    "!clinicadl quality-check pet-linear data_adni/CAPS_example data_adni/QC_result_pet.tsv 18FFDG cerebellumPons2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995c67b",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "These quality check can be really conservative and may keep some images that\n",
    "are not of good quality. You may want to check the images kept to assess if\n",
    "their quality is good enough for your application.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_T1 = pd.read_csv(\"data_oasis/QC_result_t1.tsv\", sep=\"\\t\")\n",
    "print(df_T1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a1e0e",
   "metadata": {},
   "source": [
    "Based on these TSV file, participant `OASIS10304` should be discarded for the\n",
    "rest of your analysis. If you compare its registration with [MNI152NLin2009cSym\n",
    "template](https://bids-specification.readthedocs.io/en/stable/99-appendices/08-coordinate-systems.html),\n",
    "you will see that temporal regions are misaligned as well as occipital regions\n",
    "and cerebellum leading to this low probability value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf75569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality-check for pet-linear preprocessing \n",
    "df_pet = pd.read_csv(\"data_adni/QC_result_pet.tsv\", sep=\"\\t\")\n",
    "print(df_pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a04432",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Now that you have your preprocessed data, you can split them in order to \n",
    "prepare your training in the next notebook."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
